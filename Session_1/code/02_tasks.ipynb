{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a subset of artificial intelligence that enables machines to generate new, original content such as images, music, text, or videos based on patterns learned from existing data. This technology uses complex algorithms and machine learning techniques to create novel outputs that are often indistinguishable from human-created work, with applications spanning creative industries, scientific research, and more."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM ( this port is forwarded from the docker container to the host system)\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt\n",
    "body = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Send the POST request\n",
    "response = requests.post(url=url, json=body)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method ``ollama.chat(...)``\n",
    "- First use method ``ollama.chat(...)`` with ``stream=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo! Es ist sch√∂n, dich zu sehen. Wie kann ich dir heute helfen?\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "text = \"Hallo\"\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': text,\n",
    "  },\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. Sunlight enters Earth's atmosphere and is composed of a spectrum of colors, including red, orange, yellow, green, blue, indigo, and violet.\n",
      "2. When sunlight interacts with the tiny molecules of gases in the atmosphere, such as nitrogen and oxygen, it encounters shorter (blue) wavelengths more frequently than longer (red) wavelengths.\n",
      "3. These shorter wavelengths are scattered in all directions by the gas molecules, while the longer wavelengths continue to travel in a straight line.\n",
      "4. Since the blue light is scattered in all directions, our eyes see the blue color everywhere, giving the sky its blue appearance.\n",
      "\n",
      "The reason we don't see the red and orange light as much is that the Earth's atmosphere scatters these colors less efficiently than the blue light. This is why the sky often appears more blue during the daytime when the sun is overhead, and more reddish or pinkish during sunrise and sunset, when the sunlight has to travel through more of the atmosphere.\n",
      "\n",
      "It's worth noting that the color of the sky can also be affected by other factors, such as:\n",
      "\n",
      "* Air pollution: Particles in the air can scatter light in different ways, making the sky appear more hazy or gray.\n",
      "* Dust and water vapor: Tiny particles in the air can also affect the way light is scattered, changing the apparent color of the sky.\n",
      "* Atmospheric conditions: Clouds, fog, and other weather phenomena can alter the way light interacts with the atmosphere, producing different colors and effects.\n",
      "\n",
      "But on a clear day, Rayleigh scattering remains the main reason why our sky appears blue."
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "response = ollama.chat(    \n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk[\"message\"][\"content\"],end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Dieser Film war absolut fantastisch! Die Schauspielerei war hervorragend, die Handlung fesselnd und die visuelle Gestaltung atemberaubend.\n",
      "\n",
      "Model Output:\n",
      "Es klingt, als h√§ttest du einen wirklich beeindruckenden Film gesehen!\n",
      "\n",
      "Die Beschreibung \"fantastisch\" und \"atemberaubend\" deutet darauf hin, dass der Film eine starke emotionale Wirkung auf dich hatte. Die Erw√§hnung von \"hervorragender Schauspielerei\" legt nahe, dass die Darstellerinnen und Schauspieler sehr √ºberzeugend und authentisch spielten.\n",
      "\n",
      "Die Beschreibung \"fesselnd\" f√ºr die Handlung deutet darauf hin, dass der Film dich spannte und nicht loslie√ü. Und die Bezeichnung \"visuelle Gestaltung\" suggeriert, dass das visuelle Konzept des Films sehr ansprechend und beeindruckend war.\n",
      "\n",
      "Was f√ºr einen Film es war, ist mir jedoch nicht bekannt. K√∂nntest du mir vielleicht mehr √ºber den Film erz√§hlen?\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Ich hasse dieses Produkt. Es ist eine Geldverschwendung.Sentiment: Negativ\n",
      "\n",
      "Model Output:\n",
      "Es scheint, als h√§ttest du ein sehr negatives Erlebnis mit einem Produkt gehabt und f√ºhlst dich daraufhin entt√§uscht oder sogar betrogen, weil du glaubst, dass es \"nicht wert war\" oder eine unn√∂tige Ausgabe f√ºr dein Geld gewesen ist. Dieser Gef√ºhl kann sehr frustrierend sein, besonders wenn man sich erwartet hatte, dass das Produkt seine Versprechen oder die eigenen Erwartungen erf√ºllen w√ºrde.\n",
      "\n",
      "Wie kannst ich dir helfen? Hast du spezifische Bedenken oder Fragen zum Produkt oder dem Kaufprozess?\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Das Essen war k√∂stlich und der Service war ausgezeichnet.Sentiment: Positiv\n",
      "Text: \"Ich bin so entt√§uscht von diesem Produkt. Es funktioniert nicht wie beworben.\"\n",
      "Sentiment: Negativ\n",
      "\n",
      "Model Output:\n",
      "Vielen Dank f√ºr die Beispiele!\n",
      "\n",
      "Ja, der Satz \"Das Essen war k√∂stlich und der Service war ausgezeichnet\" weist ein sehr positives Sentiment auf, da er positive Eigenschaften (k√∂stlich, ausgezeichnet) beinhaltet.\n",
      "\n",
      "Im Gegensatz dazu lautet der zweite Satz \"Ich bin so entt√§uscht von diesem Produkt. Es funktioniert nicht wie beworben\" ein sehr negatives Beispiel f√ºr einen negativen Text. Die Worte \"entt√§uscht\", \"funktioniert nicht\" und \"nicht wie beworben\" legen eine negative Stimmung nahe.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADD HERE YOUR PROMPTS\n",
    "\n",
    "zero_shot_prompt = \"\"\"Dieser Film war absolut fantastisch! Die Schauspielerei war hervorragend, die Handlung fesselnd und die visuelle Gestaltung atemberaubend.\"\"\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Ich hasse dieses Produkt. Es ist eine Geldverschwendung.Sentiment: Negativ\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Das Essen war k√∂stlich und der Service war ausgezeichnet.Sentiment: Positiv\n",
    "Text: \"Ich bin so entt√§uscht von diesem Produkt. Es funktioniert nicht wie beworben.\"\n",
    "Sentiment: Negativ\"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI uses machine learning to create new content by analyzing patterns in existing data, with applications in generating text, images, and music, and is gaining popularity in the creative industries.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Nutze daf√ºr nur emotjis\n",
      "\n",
      "Model Output:\n",
      "üòäüëçüí¨\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# ADD HERE YOUR PROMPT\n",
    "refined_prompt = \"Nutze daf√ºr nur emotjis\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "inder der auf englisch redet\n",
      "\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Ja, ich kann auf Englisch sprechen. Wie kann ich Ihnen helfen?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "\n",
    "structured_prompt = \"\"\"inder der auf englisch redet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
