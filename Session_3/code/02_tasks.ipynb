{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load huggingface embedding model (``model_name=\"sentence-transformers/all-mpnet-base-v2\"``)\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\gamemode_1\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\gamemode_1\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 768\n",
      "[-0.048951826989650726, -0.039862070232629776, -0.021562794223427773, 0.009908574633300304, -0.03810390084981918, 0.012684382498264313, 0.043494462966918945, 0.07183389365673065, 0.00974861066788435, -0.006987075321376324]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "query_vector = embedding_model.embed_query(text)\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: ``PyPDFLoader``\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "# ADD HERE YOUR CODE\n",
    "loader = PyPDFLoader(file_path=pdf_doc)\n",
    "\n",
    "# Load and split documents in chunks\n",
    "# ADD HERE YOUR CODE\n",
    "pages = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 10000, chunk_overlap = 20)\n",
    "pages_chunked = splitter.split_documents(pages)\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_text(text):\n",
    "    # Remove surrogate pairs\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_and_create_document(chunk):\n",
    "    cleaned_text = clean_text(chunk.page_content)\n",
    "    return Document(page_content=cleaned_text, metadata=chunk.metadata)\n",
    "\n",
    "\n",
    "pages_chunked_cleaned = [clean_and_create_document(chunk) for chunk in pages_chunked]\n",
    "\n",
    "\n",
    "print(pages_chunked[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.' metadata={'source': './AI_Book.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 501\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create a collection\n",
    "collection = client.get_or_create_collection(\"collection_name\")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create chromadb\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embedding_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c6faad1f-cda5-4a39-b04a-d64a223e58f5',\n",
       " 'fb520e67-2bd3-4d19-81f0-db2b3f3d2cd0',\n",
       " 'bdfce2da-329d-48d5-8bf6-1dbace21f8c5',\n",
       " 'e8cc0431-dbca-45bf-a8c1-df2fe4b91e70',\n",
       " '80eeb072-c29a-4e1d-a6ba-1999febe62f1',\n",
       " '1ba8ae1b-9b03-4694-8135-90f37f3ae0ac',\n",
       " '92e41fd1-784b-4646-815a-5e46da83faa8',\n",
       " '16232786-a5b8-4b35-b297-8b765b2bda63',\n",
       " 'ce6a86d4-0f47-4808-adee-2ee2f63e5092',\n",
       " '756c06e5-261e-4b8f-b1d7-6305c46a567a',\n",
       " '2f5aba46-ea85-418f-a735-c8fd3a61af36',\n",
       " 'de116e5a-0c9f-44af-b2e2-ea2188310a1b',\n",
       " 'cf17538f-5d64-48f8-8005-b7c639bf6e2c',\n",
       " 'bdf1abf8-b70f-4f7d-b7ad-a1e318dee66d',\n",
       " '991dd677-22ee-4b0b-995c-0cfa8f0c35b6',\n",
       " 'd8ce1875-21e5-4bdd-b4f2-ec1af1a2e4a1',\n",
       " 'f25dc4a0-aabc-4110-bb95-de9615db2b10',\n",
       " '90f6f793-b6de-482a-805d-c62da9bda6fc',\n",
       " 'd5a9c88f-73d5-4210-916f-09709c996981',\n",
       " '4730761e-e38b-44de-bc34-adca9318660d',\n",
       " 'ee72100e-3df1-4280-9172-f914c17292a8',\n",
       " 'ea7d5585-e644-4057-a26f-72bc8195bfe1',\n",
       " '0919b2ea-4d92-468a-97a3-fe8c0c3af0bc',\n",
       " '4b17753f-e99c-4d58-bb12-ee075d0c3b9e',\n",
       " 'cff6a5e8-d337-4801-a77a-a788ff22c5e1',\n",
       " 'd0a13147-b082-40a2-aa1b-71b64dcb8cb3',\n",
       " 'f4c4d686-de81-43f5-981d-764ac5e6adca',\n",
       " '90ff14f4-3e7e-437b-b540-e98de02a9bde',\n",
       " '2dbed797-3b19-463c-b82d-7ac145af9b87',\n",
       " '137cbbe5-680a-40aa-9395-e40d9f214c49',\n",
       " '3df6bcb0-48e8-4066-b56a-78279e779161',\n",
       " '19a5f285-225f-47b9-b72c-535a187df2f5',\n",
       " '44f47b2f-b180-4c5c-a0b6-11db6f1c53da',\n",
       " '857fc5ad-7b05-4814-b3b4-3281aebbd3e4',\n",
       " 'a0c1def1-b310-4df0-9a61-034221d0d938',\n",
       " '5e39246f-2e9c-48f5-a0d4-7a073caf75c2',\n",
       " 'f467423c-5685-4b10-943d-318cfe814ae6',\n",
       " '28fc6f3b-1bb2-40a6-8416-79da2696979c',\n",
       " '3291bdf5-2dee-4e31-934e-46cb0de41aa8',\n",
       " '002c3bc3-ff3d-4c00-a0c3-ea6e7f68ae12',\n",
       " '39540339-aa7f-4140-b34a-e00a77c6dc9c',\n",
       " 'b7a91e75-cd0a-4a4a-9b18-785a989dca00',\n",
       " 'ebdf7d32-e80a-4f63-8403-52dd7054d12a',\n",
       " '603971a8-44dd-4950-894f-be45ed64986e',\n",
       " 'a47c66db-73b6-4ab6-b822-4ed871251b2c',\n",
       " '8a1e073e-e9ba-4d18-895e-4b20432c7ac3',\n",
       " '1ba51e30-3af2-4e9d-ab54-2ad218d99a84',\n",
       " '7d189054-2b3d-4c7a-ace0-596e925c86b8',\n",
       " '2d88907b-3cbd-424a-90fa-adab5276976a',\n",
       " 'a824f45f-cb92-4275-a544-5be181e2faf4']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned[:50]))]\n",
    "\n",
    "vector_db_from_client.add_documents(documents=pages_chunked_cleaned[:50], id = uuids)\n",
    "\n",
    "\n",
    "#uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned[:50]))]\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "#vector_db_from_client.add_texts(pages_chunked_cleaned, id = uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame the Problem                                                                                                    39\n",
      "Select a Performance Measure                                                                                  42\n",
      "Check the Assumptions                                                                                             45\n",
      "Get the Data                                                                                                                    45\n",
      "Create the Workspace                                                                                                45\n",
      "Download the Data                                                                                                    49\n",
      "Take a Quick Look at the Data Structure                                                                50\n",
      "Create a Test Set                                                                                                          54\n",
      "Discover and Visualize the Data to Gain Insights                                                     58\n",
      "Visualizing Geographical Data                                                                                 59\n",
      "Looking for Correlations                                                                                           62\n",
      "Experimenting with Attribute Combinations                                                        65\n",
      "Prepare the Data for Machine Learning Algorithms                                                66\n",
      "Data Cleaning                                                                                                             67\n",
      "Handling Text and Categorical Attributes                                                              69\n",
      "Custom Transformers                                                                                                71\n",
      "Feature Scaling                                                                                                            72\n",
      "Transformation Pipelines                                                                                          73\n",
      "Select and Train a Model                                                                                               75\n",
      "Training and Evaluating on the Training Set                                                         75\n",
      "Better Evaluation Using Cross-Validation                                                              76\n",
      "Fine-Tune Y our Model                                                                                                  79\n",
      "Grid Search                                                                                                                 79\n",
      "Randomized Search                                                                                                   81\n",
      "Ensemble Methods                                                                                                     82\n",
      "Analyze the Best Models and Their Errors                                                             82\n",
      "Evaluate Y our System on the Test Set                                                                      83\n",
      "Launch, Monitor, and Maintain Y our System                                                            84\n",
      "Try It Out!                                                                                                                       85\n",
      "Exercises                                                                                                                          85\n",
      "3.Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\n",
      "MNIST                                                                                                                             87\n",
      "Training a Binary Classifier                                                                                          90\n",
      "Performance Measures                                                                                                  90\n",
      "Measuring Accuracy Using Cross-Validation                                                        91\n",
      "Confusion Matrix                                                                                                       92\n",
      "Precision and Recall                                                                                                   94\n",
      "Precision/Recall Tradeoff                                                                                          95\n",
      "The ROC Curve                                                                                                          99\n",
      "Multiclass Classification                                                                                             102\n",
      "Error Analysis                                                                                                              104\n",
      "iv | Table of Contents\n",
      "{'page': 5, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "CHAPTER 1\n",
      "The Machine Learning Landscape\n",
      "With Early Release ebooks, you get books in their earliest form\n",
      "the authors raw and unedited content as he or she writesso you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 1 in the final\n",
      "release of the book.\n",
      "When most people hear Machine Learning,  they picture a robot: a dependable but\n",
      "ler or a deadly Terminator depending on who you ask. But Machine Learning is not\n",
      "just a futuristic fantasy, its already here. In fact, it has been around for decades in\n",
      "some specialized applications, such as Optical Character Recognition  (OCR). But the\n",
      "first ML application that really became mainstream, improving the lives of hundreds\n",
      "of millions of people, took over the world back in the 1990s: it was the spam filter .\n",
      "Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning\n",
      "(it has actually learned so well that you seldom need to flag an email as spam any\n",
      "more). It was followed by hundreds of ML applications that now quietly power hun\n",
      "dreds of products and features that you use regularly, from better recommendations\n",
      "to voice search.\n",
      "Where does Machine Learning start and where does it end? What exactly does it\n",
      "mean for a machine to learn  something? If I download a copy of Wikipedia, has my\n",
      "computer really learned something? Is it suddenly smarter? In this chapter we will\n",
      "start by clarifying what Machine Learning is and why you may want to use it.\n",
      "Then, before we set out to explore the Machine Learning continent, we will take a\n",
      "look at the map and learn about the main regions and the most notable landmarks:\n",
      "supervised versus unsupervised learning, online versus batch learning, instance-\n",
      "based versus model-based learning. Then we will look at the workflow of a typical ML\n",
      "project, discuss the main challenges you may face, and cover how to evaluate and\n",
      "fine-tune a Machine Learning system.\n",
      "3\n",
      "{'page': 28, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "results. To reduce this risk, you need to monitor your system closely and promptly\n",
      "switch learning off (and possibly revert to a previously working state) if you detect a\n",
      "drop in performance. Y ou may also want to monitor the input data and react to\n",
      "abnormal data (e.g., using an anomaly detection algorithm).\n",
      "Instance-Based Versus Model-Based Learning\n",
      "One more way to categorize Machine Learning systems is by how they generalize .\n",
      "Most Machine Learning tasks are about making predictions. This means that given a\n",
      "number of training examples, the system needs to be able to generalize to examples it\n",
      "has never seen before. Having a good performance measure on the training data is\n",
      "good, but insufficient; the true goal is to perform well on new instances.\n",
      "There are two main approaches to generalization: instance-based learning and\n",
      "model-based learning.\n",
      "Instance-based learning\n",
      "Possibly the most trivial form of learning is simply to learn by heart. If you were to\n",
      "create a spam filter this way, it would just flag all emails that are identical to emails\n",
      "that have already been flagged by usersnot the worst solution, but certainly not the\n",
      "best.\n",
      "Instead of just flagging emails that are identical to known spam emails, your spam\n",
      "filter could be programmed to also flag emails that are very similar to known spam\n",
      "emails. This requires a measure of similarity  between two emails. A (very basic) simi\n",
      "larity measure between two emails could be to count the number of words they have\n",
      "in common. The system would flag an email as spam if it has many words in com\n",
      "mon with a known spam email.\n",
      "This is called instance-based learning : the system learns the examples by heart, then\n",
      "generalizes to new cases by comparing them to the learned examples (or a subset of\n",
      "them), using a similarity measure. For example, in Figure 1-15  the new instance\n",
      "would be classified as a triangle because the majority of the most similar instances\n",
      "belong to that class.\n",
      "18 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 43, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = str(\"Types of Machine Learning Systems\")\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\n",
    "    \"search_query\",\n",
    "    k=3,\n",
    "    )\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
