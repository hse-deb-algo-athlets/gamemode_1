{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple Chain with Retrieval\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a simple RAG chain with ChatOllama, HuggingFaceEmbeddings and Chroma. \n",
    "\n",
    "Process: \n",
    "\n",
    "1. Retrieve documents from chroma db based on query\n",
    "2. Invoke chain with retrieved documents as input\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load llm model via ollama\n",
    "- load huggingface embedding model (``model_name=\"sentence-transformers/all-mpnet-base-v2\"``)\n",
    "- create chroma db client\n",
    "- create prompt template for summarization\n",
    "- create simple chain with following steps: retrieved documents, prompt, model, output parser\n",
    "- create query and perform similarity search with a query\n",
    "- invoke chain and pass retrieved documents to the chain\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 8,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "model = \"llama3.2\"\n",
    "llm = ChatOllama(model=model)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "c:\\gamemode_1\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\gamemode_1\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
=======
      "c:\\Users\\felix\\OneDrive\\Studium\\3. Semester\\Technische Informatik\\labor\\gamemode_1\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\felix\\OneDrive\\Studium\\3. Semester\\Technische Informatik\\labor\\gamemode_1\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "# ADD HERE YOUR CODE\n",
    "collection = client.get_or_create_collection(\"collection_name\")\n",
    "\n",
    "# Create chromadb\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embedding_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 9,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 13,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 33, 'source': './AI_Book.pdf'}, page_content='Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\nWhether or not they are trained with human supervision (supervised, unsuper\\nvised, semisupervised, and Reinforcement Learning)\\nWhether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\nWhether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLets look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape'), Document(metadata={'page': 40, 'source': './AI_Book.pdf'}, page_content='Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMinds AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15'), Document(metadata={'page': 44, 'source': './AI_Book.pdf'}, page_content='Figure 1-15. Instance-based learning\\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of these exam\\nples, then use that model to make predictions . This is called model-based learning\\n(Figure 1-16 ).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you down\\nload the Better Life Index  data from the OECDs website  as well as stats about GDP\\nper capita from the IMFs website . Then you join the tables and sort by GDP per cap\\nita. Table 1-1  shows an excerpt of what you get.\\nTypes of Machine Learning Systems | 19'), Document(metadata={'page': 28, 'source': './AI_Book.pdf'}, page_content='CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear Machine Learning,  they picture a robot: a dependable but\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, its already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any\\nmore). It was followed by hundreds of ML applications that now quietly power hun\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really learned something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3')]\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "docs = vector_db_from_client.similarity_search(search_query)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 14,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'The main themes in these retrieved documents are:\\n\\n1. **Classification of Machine Learning Systems**: The documents discuss the different types of machine learning systems based on four criteria:\\n * Supervised vs unsupervised learning\\n * Online vs batch learning\\n * Instance-based vs model-based learning\\n2. **Types of Learning**: The documents explain the different types of learning, including:\\n * Supervised learning: where the training data includes labeled examples (e.g., spam classification)\\n * Unsupervised learning: where no labels are provided (e.g., clustering, dimensionality reduction)\\n * Semisupervised learning: a combination of supervised and unsupervised learning\\n * Reinforcement Learning: where the system learns by interacting with an environment and receiving feedback (e.g., playing games like Go)\\n3. **Learning Modes**: The documents discuss two types of learning modes:\\n * Batch learning: where the system is trained using all available data at once\\n * Online learning: where the system learns incrementally from a stream of incoming data\\n4. **Model-based vs Instance-based Learning**: The documents explain the difference between model-based and instance-based learning, which are two approaches to generalizing from examples:\\n * Model-based learning: building a model of the examples and using it to make predictions\\n * Instance-based learning: making predictions based on individual instances rather than building a model\\n5. **Machine Learning Applications**: The documents mention various machine learning applications, including spam filters, OCR, recommendations, and voice search.\\n\\nOverall, these documents provide an overview of the different types of machine learning systems, learning modes, and approaches to generalization from examples, as well as some examples of machine learning applications.'"
      ]
     },
     "execution_count": 7,
=======
       "\"The main themes in these retrieved docs are:\\n\\n1. **Types of Machine Learning Systems**: The documents categorize machine learning systems based on supervision, online vs batch learning, instance-based vs model-based learning.\\n\\n2. **Supervised/Unsupervised Learning**: Four major categories of machine learning systems are:\\n   - Supervised learning: uses labeled data to train models.\\n   - Unsupervised learning: does not use labeled data and aims to discover patterns or relationships in the data.\\n   - Semisupervised learning: combines labeled and unlabeled data for training.\\n   - Reinforcement Learning: involves an agent that learns by interacting with an environment.\\n\\n3. **Batch vs Online Learning**: Machine learning systems can be classified as either batch learning (requires training on all available data) or online learning (can learn incrementally from a stream of incoming data).\\n\\n4. **Instance-based vs Model-based Learning**: Two approaches to generalizing from examples:\\n   - Instance-based learning: builds models based on individual instances.\\n   - Model-based learning: builds models that can be used for prediction.\\n\\n5. **Overview of Machine Learning**: The documents introduce the concept of machine learning, its history, and its applications in various products and features.\\n\\n6. **Clarifying Machine Learning Concepts**: The chapter aims to clarify what machine learning is, how it works, and why it's useful.\""
      ]
     },
     "execution_count": 14,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Q&A with RAG\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a Q/A retrieval chain with ChatOllama, HuggingFaceEmbeddings and Chroma\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create RAG-Q/A prompt template\n",
    "- create retriever from vector db client (instead of manually passing in docs, we automatically retrieve them from our vector store based on the user question)\n",
    "- create simple chain with following steps: retriever, formatting retrieved docs, user question, prompt, model, output parser\n",
    "- create question for Q/A retrieval chain\n",
    "- invoke chain and with question\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 15,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "retriever = vector_db_from_client.as_retriever()\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain = ({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000025B0200E5D0>)\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n<context>\\n{context}\\n</context>\\n\\nAnswer the following question:\\n\\n{question}\"))])\n",
       "| ChatOllama(model='llama3.2', _client=<ollama._client.Client object at 0x0000025B24285510>, _async_client=<ollama._client.AsyncClient object at 0x0000025B25519BD0>)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_rag_chain"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 17,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'In supervised learning, the training data fed to the algorithm includes the desired solutions, called labels. This means that the machine learning model is trained on labeled data, allowing it to learn from examples and make predictions on new, unseen data. The goal of supervised learning is for the model to accurately predict the correct output or class label given the input data.'"
      ]
     },
     "execution_count": 10,
=======
       "'Supervised learning is a type of Machine Learning where the training data includes the desired solutions, called labels. This means that the algorithm is provided with examples of input data along with their corresponding correct outputs or labels, allowing it to learn from these examples and make predictions on new, unseen data.'"
      ]
     },
     "execution_count": 17,
>>>>>>> e9f30c1ad093da6ab2cf545f9ee4d026cf734444
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is supervised learning?\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
